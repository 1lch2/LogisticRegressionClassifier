{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the package and extract all the testing and training data in to different NumPy matrices.<br/>\n",
    "Calculate the number of total different labels, store the number into variable C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import h5py into numpy matrices\n",
    "with h5py.File('./data/train/images_training.h5', 'r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./data/train/labels_training.h5', 'r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "with h5py.File('./data/test/images_testing.h5', 'r') as T:\n",
    "    data_test = np.copy(T['datatest'])\n",
    "with h5py.File('./data/test/labels_testing_2000.h5', 'r') as T:\n",
    "    label_test = np.copy(T['labeltest'])\n",
    "\n",
    "# Number of different classes.\n",
    "labels = np.unique(label_train)\n",
    "C = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the input data by adding an extra column with 1 to input X martix before run the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input data.\n",
    "def preprocess(X):\n",
    "    temp = np.ones((X.shape[0], 1))\n",
    "    X_ = np.c_[temp, X]\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax function.\n",
    "$$Softmax = \\sigma({W^T}X) = \\frac{e^{{W^T}X}}{\\displaystyle\\sum^{n}_{k=1}{e^{{W^T}X}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftMax function.\n",
    "def softmax(wx):\n",
    "    e_wx = np.exp(wx)\n",
    "    sfm = e_wx / e_wx.sum(axis=1, keepdims=True)\n",
    "    return sfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Cross-Entropy loss with regularization parameter.\n",
    "$$CrossEntropy = - \\displaystyle \\sum^{n}_{i=1}{y_i\\log(\\sigma({{w_i}^T}x_i))} + \\frac{\\lambda}{2N}\\displaystyle\\sum^{n}_{i=1}{{w_i}^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function with regularization term.\n",
    "def cross_entropy(X, y, W, lmd=0.01):\n",
    "    P = softmax(X.dot(W))\n",
    "    pred_prob = np.log(P)\n",
    "    N = pred_prob.shape[0]\n",
    "    row_n = range(N)\n",
    "\n",
    "    Y = np.zeros((N, C))  # N x C\n",
    "    Y[row_n, y] = 1\n",
    "\n",
    "    loss_sum = 0\n",
    "    for row_p, row_y in zip(pred_prob, Y):\n",
    "        loss_sum += np.vdot(row_p, row_y)\n",
    "    \n",
    "    # Regularization term.\n",
    "    R = (lmd / 2*N) * np.sum(np.square(W[1:]))\n",
    "    return -(loss_sum / N) + R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the gradient with regularization parameter.\n",
    "$$gradient = \\bigtriangledown =  - \\frac{1}{n} \\big( \\sigma({W^T}X)-Y \\big)X + \\frac{\\lambda}{N}W[1:]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gradient of loss function with regularization term.\n",
    "def gradient(X, y, W, lmd=0.01):\n",
    "    # Calculate the conditianal probability using softmax function.\n",
    "    try:\n",
    "        P = softmax(X.dot(W))  # N x C\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print('Please check the input data.')\n",
    "    else:\n",
    "        N = P.shape[0]\n",
    "        row_n = range(N)\n",
    "\n",
    "        # Vectorize the label array, each row has C columns.\n",
    "        # a.k.a. OneHot encoding.\n",
    "        Y = np.zeros((N, C))  # N x C\n",
    "        Y[row_n, y] = 1\n",
    "\n",
    "        P_Y = P - Y  # A - Y\n",
    "        gre = X.T.dot(P_Y) / N  # K x C, same with W.\n",
    "        \n",
    "        # Regularization term.\n",
    "        R = (lmd/N) * (np.r_[np.zeros((1, W.shape[1])), W[1:]])\n",
    "        return gre + R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mini-batch gredient descent to update the weight parameter.\n",
    "$$W_{i+1} = W_i - \\eta\\bigtriangledown $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch Gradient Descent method.\n",
    "def mini_batch_GD(X, y, W, lr=0.005, iterlimit=100, batch_size=10, lmd=0.01):\n",
    "    W_old = W.copy()\n",
    "    itercount = 0\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Record loss history.\n",
    "    loss_hist = [cross_entropy(X, y, W, lmd)] \n",
    "    nbatches = int(np.ceil(float(N) / batch_size)) \n",
    "    \n",
    "    #Stochastic Gradient Descent.\n",
    "    while itercount < iterlimit:\n",
    "        itercount += 1\n",
    "        mix_ids = np.random.permutation(N)  # Randomize the smaller datasets.\n",
    "\n",
    "        # Batch Gradient Descent.\n",
    "        for i in range(nbatches):\n",
    "            batch_ids = mix_ids[batch_size * i : min(batch_size * (i + 1), N)]\n",
    "            X_batch, y_batch = X[batch_ids], y[batch_ids]\n",
    "            W -= lr * gradient(X_batch, y_batch, W, lmd)\n",
    "            \n",
    "        # Record the loss of every iteration.\n",
    "        loss_hist.append(cross_entropy(X, y, W, lmd))\n",
    "        \n",
    "        # Jump out of loop when weight parameters converge.\n",
    "        delta = np.linalg.norm(W - W_old)\n",
    "        if np.sqrt(delta) < 1e-5:\n",
    "            print('Converged.\\n')\n",
    "            break\n",
    "        W_old = W.copy()\n",
    "    return W, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the labels and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels.\n",
    "def predict(W, X):\n",
    "    out = softmax(X.dot(W))\n",
    "    return np.argmax(out, axis=1)\n",
    "\n",
    "# Calculate total accuracy.\n",
    "def accuracy(y_pred, y_ture):\n",
    "    try:\n",
    "        out = sum(y_pred == y_ture)\n",
    "        ratio = out / len(y_pred)\n",
    "    except TypeError as e:\n",
    "        print(format(e))\n",
    "        print('Please check arugments in fit method.')\n",
    "    else:\n",
    "        return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the fit and predict part into a sigle module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(para):\n",
    "    data_train_ = preprocess(data_train)\n",
    "    data_test_ = preprocess(data_test)\n",
    "    \n",
    "    # Record the training time.\n",
    "    time_start = time.time()\n",
    "    W_init = np.random.randn(data_train_.shape[1], 10)\n",
    "    W, loss_hist = mini_batch_GD(data_train_, label_train, W_init, batch_size=20, iterlimit=800, lr=0.005, lmd=para)\n",
    "    time_end = time.time()\n",
    "\n",
    "    label_predict = predict(W, data_test_)\n",
    "    print(\"Accuracy of model on test set: {:.2%}\".format(accuracy(label_predict, label_test[:2000])))\n",
    "    print(\"Time: {:.3f} s.\".format(time_end - time_start))\n",
    "    \n",
    "    acc = accuracy(label_predict, label_test[:2000])\n",
    "    loss_avg = np.mean(loss_hist[-5])\n",
    "    \n",
    "    return loss_avg, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the line chart of accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune():\n",
    "    paralist = [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    losslist = []\n",
    "    acclist = []\n",
    "\n",
    "    for para in paralist:\n",
    "        l, a = run(para)\n",
    "        losslist.append(l)\n",
    "        acclist.append(a)\n",
    "    \n",
    "    # Draw the line chart of lambda and loss.\n",
    "    plt.plot(paralist, losslist)\n",
    "    plt.xlabel('learning rate', fontsize=12)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    # Draw the line chart of lambda and accuracy.\n",
    "    plt.plot(paralist, acclist)\n",
    "    plt.xlabel('learning rate', fontsize=12)\n",
    "    plt.ylabel('accuracy', fontsize=12)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifier on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the code and write predicted labels to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input data.\n",
    "data_train_ = preprocess(data_train)\n",
    "data_test_ = preprocess(data_test[:5000])\n",
    "\n",
    "# Record the training time.\n",
    "time_start = time.time()\n",
    "W_init = np.random.randn(data_train_.shape[1], 10)\n",
    "W, _losshistory = mini_batch_GD(data_train_, label_train, W_init, batch_size=20, iterlimit=800, lr=0.005, lmd=0.01)\n",
    "time_end = time.time()\n",
    "\n",
    "# Make predictions.\n",
    "label_predict = predict(W, data_test_)\n",
    "\n",
    "# Write prediction into file.\n",
    "h5file = h5py.File('./Output/predicted_labels.h5', 'w')\n",
    "h5file.create_dataset('output', data=label_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code performance.\n",
    "Predicting the 5000 rows of test_data.\n",
    "- Runtime: 302s\n",
    "\n",
    "Accuracy on the first 2000 rows of test_data.\n",
    "- Accuracy: 85%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}